{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGg3bLLQhTH-"
   },
   "source": [
    "# $$CatBoost\\ Object\\ Importance\\ Tutorial$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXXqGWbEhTIB"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catboost/tutorials/blob/master/model_analysis/object_importance_tutorial.ipynb)\n",
    "\n",
    "#### In this tutorial we show how you can detect noisy objects in your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2zGCRcGzhTIB"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import catboost\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import Pool, cv\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import StepLR,MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import Recall\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "path_dir=os.getcwd()\n",
    "path_train='../train_dataset_train.csv'\n",
    "path_test='../test_dataset_test.csv'\n",
    "path_subm='../Иннополис/sample_solution.csv'\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    RS: int = 42\n",
    "    treshold: float = 0.0\n",
    "    device='cpu'\n",
    "    num_epochs: int = 120\n",
    "    batch_size: int = 256\n",
    "    num_workers: int = 0\n",
    "        \n",
    "def get_df(path,train=True):\n",
    "    df=pd.read_csv(path)\n",
    "    df=df.sort_index(axis=1)\n",
    "    df.drop(['id','.geo','area'],axis=1,inplace=True)\n",
    "    if train:\n",
    "        df=df.loc[df.crop.isin([0,4])]\n",
    "        y=df[['crop']]\n",
    "        df.drop(['crop'],axis=1,inplace=True)  \n",
    "    \n",
    "    df.rename({c:pd.to_datetime(c.replace('nd_mean_','')) for c in df.columns},axis=1,inplace=True) \n",
    "    df[df<=0]=0    \n",
    "    return (df.reset_index(drop=True).values,y.crop.reset_index(drop=True).values) if train else df.reset_index(drop=True).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ogus18M6hTIC"
   },
   "source": [
    "#### First, let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "D4ZlNH8HhTIC",
    "outputId": "fd795977-cd89-4327-d5fb-b937c104560d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1299, 70) (145, 70)\n"
     ]
    }
   ],
   "source": [
    "X, y =  df,y=get_df(path_train)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "train_pool = Pool(X_train, y_train)\n",
    "validation_pool = Pool(X_validation, y_validation)\n",
    "\n",
    "print(train_pool.shape, validation_pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8THh6FDhTID"
   },
   "source": [
    "#### Let's train CatBoost on clear data and take a look at the quality. We set a small learning rate to avoid overfitting when we start removing noisy objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_model={\n",
    "    'loss_function':'Logloss',\n",
    "    'random_seed':42,\n",
    "    'eval_metric':'Recall',\n",
    "    'bootstrap_type':'Bernoulli',#'Bayesian',#'Bernoulli',#'Poisson',\n",
    "    'l2_leaf_reg': 3,\n",
    "    'early_stopping_rounds':4,\n",
    "    'iterations':50,         \n",
    "     'verbose':False,\n",
    "    'depth': 1,\n",
    "    'learning_rate': 0.3,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ltWyaDxMhTID",
    "outputId": "497d619c-ebae-45c1-a65f-ee7678434097"
   },
   "outputs": [],
   "source": [
    "indices, scores = cb.get_object_importance(\n",
    "    train_pool,\n",
    "    validation_pool,\n",
    "    #importance_values_sign='Negative',\n",
    "    importance_values_sign='Positive'\n",
    "    # Positive values means that the optimized metric\n",
    "    # value is increase because of given train objects.\n",
    "    # So here we get the indices of bad train objects.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P6aw999hTIE"
   },
   "source": [
    "#### Let's inject random noise into 10% of training labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxrB34FthTIF"
   },
   "source": [
    "#### And train CatBoost on noisy data and take a look at the quality:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlz-QY7bhTIF"
   },
   "source": [
    "#### Now let's sample random 500 validate objects (because counting object importance on the entire validation dataset can take a long time) and calculate the train objects importance for these validation objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_Ri2RbaHhTIG"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_validation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m validation_pool_sampled \u001b[38;5;241m=\u001b[39m Pool(X_validation[test_idx], y_validation[test_idx], cat_features\u001b[38;5;241m=\u001b[39mcat_features)\n\u001b[0;32m      5\u001b[0m indices, scores \u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mget_object_importance(\n\u001b[0;32m      6\u001b[0m     validation_pool_sampled,\n\u001b[0;32m      7\u001b[0m     train_pool_noisy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m                                       \u001b[38;5;66;03m# So here we get the indices of bad train objects.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mmtrand.pyx:965\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "test_idx = np.random.choice(np.arange(y_validation.shape[0]), size=500, replace=False)\n",
    "validation_pool_sampled = Pool(X_validation[test_idx], y_validation[test_idx], cat_features=cat_features)\n",
    "\n",
    "indices, scores = cb.get_object_importance(\n",
    "    validation_pool_sampled,\n",
    "    train_pool_noisy,\n",
    "    importance_values_sign='Positive' # Positive values means that the optimized metric\n",
    "                                      # value is increase because of given train objects.\n",
    "                                      # So here we get the indices of bad train objects.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK2ui3owhTIG"
   },
   "source": [
    "#### Finally, in a loop, let's remove noisy objects in batches, retrain the model, and see how the quality on the test dataset improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwkCnBrnhTIG",
    "outputId": "697d6009-face-4ef4-c1f2-ea47863acbbd"
   },
   "outputs": [],
   "source": [
    "def train_and_print_score(train_indices, remove_object_count):\n",
    "    cb.fit(X_train[train_indices], y_train_noisy[train_indices], cat_features=cat_features)\n",
    "    metric_value = cb.eval_metrics(validation_pool, ['RMSE'])['RMSE'][-1]\n",
    "    s = 'RMSE on validation datset when {} harmful objects from train are dropped: {}'\n",
    "    print(s.format(remove_object_count, metric_value))\n",
    "\n",
    "batch_size = 250\n",
    "train_indices = np.full(X_train.shape[0], True)\n",
    "train_and_print_score(train_indices, 0)\n",
    "for batch_start_index in range(0, 2000, batch_size):\n",
    "    train_indices[indices[batch_start_index:batch_start_index + batch_size]] = False\n",
    "    train_and_print_score(train_indices, batch_start_index + batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws6QXJjzhTIG"
   },
   "source": [
    "#### Therefore, we have the following RMSE values on the validation dataset:\n",
    "    \n",
    "||RMSE on the validation dataset|\n",
    "|-|-|\n",
    "|Clear train dataset: | 0.22947301323494568|\n",
    "|Noisy train dataset: | 0.24770929523786442|\n",
    "|Purified train dataset: | 0.231598588484771|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR_8IASthTIH"
   },
   "source": [
    "#### $$So\\ now\\ you\\ can\\ try\\ to\\ clear\\ the\\ train\\ dataset\\ of\\ noisy\\ objects\\ and\\ get\\ better\\ quality!$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
